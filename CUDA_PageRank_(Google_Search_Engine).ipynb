{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anshulk-cmu/CUDA_PageRank/blob/main/CUDA_PageRank_(Google_Search_Engine).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPt9kQPtwzgZ",
        "outputId": "9cfa51ee-83d0-46d3-9af9-b2b96f1f8ae0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Timestamp: 2025-09-28T03:50:36.583735+00:00Z\n",
            "Python: 3.12.11  OS: Linux-6.6.97+-x86_64-with-glibc2.35\n",
            "=== nvidia-smi ===\n",
            "Sun Sep 28 03:50:36 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "=== nvcc --version ===\n",
            "nvcc: NVIDIA (R) Cuda compiler driver\n",
            "Copyright (c) 2005-2024 NVIDIA Corporation\n",
            "Built on Thu_Jun__6_02:18:23_PDT_2024\n",
            "Cuda compilation tools, release 12.5, V12.5.82\n",
            "Build cuda_12.5.r12.5/compiler.34385749_0\n",
            "PyTorch: 2.8.0+cu126\n",
            "torch.cuda.is_available: True\n",
            "CUDA device count: 1\n",
            "Current device index: 0\n",
            "Device name: Tesla T4\n",
            "Compute capability: 7.5\n",
            "Total memory (GB): 14.74\n",
            "CUDA runtime version (torch): 12.6\n"
          ]
        }
      ],
      "source": [
        "# GPU env check & log (Colab)\n",
        "import os, sys, subprocess, datetime, platform\n",
        "log = []\n",
        "\n",
        "from datetime import datetime, UTC\n",
        "ts = datetime.now(UTC).isoformat()\n",
        "\n",
        "def run(cmd):\n",
        "    try:\n",
        "        out = subprocess.check_output(cmd, shell=True, stderr=subprocess.STDOUT, text=True)\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        out = e.output\n",
        "    return out.strip()\n",
        "\n",
        "log.append(f\"Timestamp: {datetime.now(UTC).isoformat()}Z\")\n",
        "log.append(f\"Python: {sys.version.split()[0]}  OS: {platform.platform()}\")\n",
        "\n",
        "# nvidia-smi\n",
        "log.append(\"=== nvidia-smi ===\")\n",
        "log.append(run(\"nvidia-smi || echo 'nvidia-smi not found'\"))\n",
        "\n",
        "# nvcc\n",
        "log.append(\"=== nvcc --version ===\")\n",
        "log.append(run(\"nvcc --version || echo 'nvcc not found'\"))\n",
        "\n",
        "# PyTorch CUDA\n",
        "try:\n",
        "    import torch\n",
        "    log.append(f\"PyTorch: {torch.__version__}\")\n",
        "    log.append(f\"torch.cuda.is_available: {torch.cuda.is_available()}\")\n",
        "    if torch.cuda.is_available():\n",
        "        i = torch.cuda.current_device()\n",
        "        props = torch.cuda.get_device_properties(i)\n",
        "        gb = props.total_memory / (1024**3)\n",
        "        log.append(f\"CUDA device count: {torch.cuda.device_count()}\")\n",
        "        log.append(f\"Current device index: {i}\")\n",
        "        log.append(f\"Device name: {torch.cuda.get_device_name(i)}\")\n",
        "        log.append(f\"Compute capability: {props.major}.{props.minor}\")\n",
        "        log.append(f\"Total memory (GB): {gb:.2f}\")\n",
        "        log.append(f\"CUDA runtime version (torch): {torch.version.cuda}\")\n",
        "    else:\n",
        "        log.append(\"CUDA not available in this runtime. Enable GPU in Runtime → Change runtime type.\")\n",
        "except Exception as e:\n",
        "    log.append(f\"PyTorch check failed: {e}\")\n",
        "\n",
        "# Print and save\n",
        "text = \"\\n\".join(log)\n",
        "print(text)\n",
        "with open(\"/content/gpu_env_log.txt\", \"w\") as f:\n",
        "    f.write(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkKVtUAzxrqi",
        "outputId": "74499ed8-20ad-493d-c00d-5b6ba3799588"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/cuda-pagerank/data\n",
            "--2025-09-28 03:51:11--  https://snap.stanford.edu/data/web-Google.txt.gz\n",
            "Resolving snap.stanford.edu (snap.stanford.edu)... 171.64.75.80\n",
            "Connecting to snap.stanford.edu (snap.stanford.edu)|171.64.75.80|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 21168784 (20M) [application/x-gzip]\n",
            "Saving to: ‘web-Google.txt.gz’\n",
            "\n",
            "web-Google.txt.gz   100%[===================>]  20.19M  5.24MB/s    in 6.8s    \n",
            "\n",
            "2025-09-28 03:51:19 (2.97 MB/s) - ‘web-Google.txt.gz’ saved [21168784/21168784]\n",
            "\n",
            "# Directed graph (each unordered pair of nodes is saved once): web-Google.txt \n",
            "# Webgraph from the Google programming contest, 2002\n",
            "# Nodes: 875713 Edges: 5105039\n",
            "# FromNodeId\tToNodeId\n",
            "0\t11342\n",
            "5105039\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "!mkdir -p /content/drive/MyDrive/cuda-pagerank/data\n",
        "%cd /content/drive/MyDrive/cuda-pagerank/data\n",
        "\n",
        "# Download SNAP web-Google (directed web graph)\n",
        "!wget -nc https://snap.stanford.edu/data/web-Google.txt.gz\n",
        "!gunzip -kf web-Google.txt.gz\n",
        "\n",
        "# Quick sanity: show first lines and basic counts\n",
        "!head -n 5 web-Google.txt\n",
        "!grep -v '^\\s*#' web-Google.txt | wc -l"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LJcRGaNx_W_",
        "outputId": "a37b4d91-633b-4f1b-dc54-bb8ee5a2d1ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "  \"nodes\": 916428,\n",
            "  \"edges\": 5105039,\n",
            "  \"dangling_count\": 176974,\n",
            "  \"sample_row0_len\": 212,\n",
            "  \"saved\": \"/content/drive/MyDrive/cuda-pagerank/prep/in_csr_webGoogle.npz\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Build in-neighbor CSR (rows = dest nodes, cols = source nodes), plus out-degree\n",
        "import numpy as np, os, gzip, io, time, json\n",
        "\n",
        "data_path = \"/content/drive/MyDrive/cuda-pagerank/data/web-Google.txt\"\n",
        "assert os.path.exists(data_path)\n",
        "\n",
        "# 1) read edges (skip '#')\n",
        "src = []\n",
        "dst = []\n",
        "with open(data_path, \"r\") as f:\n",
        "    for line in f:\n",
        "        if line.startswith(\"#\"):\n",
        "            continue\n",
        "        a,b = line.strip().split()\n",
        "        src.append(int(a)); dst.append(int(b))\n",
        "src = np.array(src, dtype=np.int64)\n",
        "dst = np.array(dst, dtype=np.int64)\n",
        "\n",
        "N = int(max(src.max(), dst.max())) + 1\n",
        "M = len(src)\n",
        "\n",
        "# 2) out-degree (for weights 1/outdeg)\n",
        "outdeg = np.bincount(src, minlength=N).astype(np.int64)\n",
        "\n",
        "# 3) in-CSR row_ptr/col_idx/val (val = 1/outdeg(source); dangling nodes => 0 later)\n",
        "row_ptr = np.zeros(N + 1, dtype=np.int64)\n",
        "np.add.at(row_ptr, dst + 1, 1)\n",
        "np.cumsum(row_ptr, out=row_ptr)\n",
        "\n",
        "col_idx = np.empty(M, dtype=np.int64)\n",
        "val = np.empty(M, dtype=np.float64)\n",
        "\n",
        "# fill per row using a cursor\n",
        "cursor = row_ptr[:-1].copy()\n",
        "w = 1.0 / np.maximum(outdeg, 1)  # avoid div-by-zero\n",
        "for s, d in zip(src, dst):\n",
        "    i = cursor[d]\n",
        "    col_idx[i] = s\n",
        "    val[i] = w[s]\n",
        "    cursor[d] = i + 1\n",
        "\n",
        "# optional: sort columns within each row for better coalescing later\n",
        "for r in range(N):\n",
        "    start, end = row_ptr[r], row_ptr[r+1]\n",
        "    if end - start > 1:\n",
        "        idx = np.argsort(col_idx[start:end], kind=\"mergesort\")\n",
        "        col_idx[start:end] = col_idx[start:end][idx]\n",
        "        val[start:end] = val[start:end][idx]\n",
        "\n",
        "# persist\n",
        "outdir = \"/content/drive/MyDrive/cuda-pagerank/prep\"\n",
        "os.makedirs(outdir, exist_ok=True)\n",
        "np.savez_compressed(f\"{outdir}/in_csr_webGoogle.npz\",\n",
        "                    N=N, M=M, row_ptr=row_ptr, col_idx=col_idx, val=val, outdeg=outdeg)\n",
        "\n",
        "# quick log\n",
        "print(json.dumps({\n",
        "    \"nodes\": int(N),\n",
        "    \"edges\": int(M),\n",
        "    \"dangling_count\": int((outdeg==0).sum()),\n",
        "    \"sample_row0_len\": int(row_ptr[1]-row_ptr[0]),\n",
        "    \"saved\": f\"{outdir}/in_csr_webGoogle.npz\"\n",
        "}, indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjayqW5xyPeX",
        "outputId": "21e9c786-bc42-46f0-abe3-2bdba5b06bc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Running CPU PageRank Baseline ===\n",
            "CPU method completed in 62 iterations.\n",
            "Total time: 210301.528 ms\n",
            "env: CPU_RESULTS={\"method\": \"CPU (NumPy)\", \"iterations\": 62, \"total_ms\": 210301.5275001526, \"ms_per_iter\": 3391.960120970203}\n"
          ]
        }
      ],
      "source": [
        "# CPU PageRank (power method, minimal)\n",
        "import numpy as np, time, json\n",
        "\n",
        "print(\"=== Running CPU PageRank Baseline ===\")\n",
        "\n",
        "# Load pre-processed graph data\n",
        "npz = np.load(\"/content/drive/MyDrive/cuda-pagerank/prep/in_csr_webGoogle.npz\", allow_pickle=False)\n",
        "N = int(npz[\"N\"]); M = int(npz[\"M\"])\n",
        "row_ptr = npz[\"row_ptr\"]; col_idx = npz[\"col_idx\"]; val = npz[\"val\"]; outdeg = npz[\"outdeg\"]\n",
        "\n",
        "# Algorithm parameters\n",
        "alpha = 0.85\n",
        "tol = 1e-6\n",
        "max_iter = 200\n",
        "\n",
        "# Initialize ranks\n",
        "r = np.full(N, 1.0 / N, dtype=np.float64)\n",
        "r_new = np.empty_like(r)\n",
        "\n",
        "dangling_mask = (outdeg == 0)\n",
        "\n",
        "# CSR matrix-vector multiplication function\n",
        "def spmv_pull(row_ptr, col_idx, val, x):\n",
        "    y = np.zeros_like(x)\n",
        "    for i in range(N):\n",
        "        s = 0.0\n",
        "        start, end = row_ptr[i], row_ptr[i+1]\n",
        "        for p in range(start, end):\n",
        "            s += val[p] * x[col_idx[p]]\n",
        "        y[i] = s\n",
        "    return y\n",
        "\n",
        "# Main iteration loop\n",
        "t0 = time.time()\n",
        "for it in range(1, max_iter + 1):\n",
        "    # Perform the SpMV operation\n",
        "    tmp = spmv_pull(row_ptr, col_idx, val, r)\n",
        "\n",
        "    # Handle dangling nodes and apply the PageRank formula\n",
        "    dangling_mass = r[dangling_mask].sum() / N\n",
        "    r_new[:] = alpha * (tmp + dangling_mass) + (1.0 - alpha) / N\n",
        "\n",
        "    # Check for convergence\n",
        "    diff = np.abs(r_new - r).sum()\n",
        "    r, r_new = r_new, r\n",
        "    if diff < tol:\n",
        "        break\n",
        "t1 = time.time()\n",
        "\n",
        "# Store results in a JSON string for later analysis\n",
        "cpu_results = {\n",
        "    \"method\": \"CPU (NumPy)\",\n",
        "    \"iterations\": it,\n",
        "    \"total_ms\": (t1 - t0) * 1000,\n",
        "    \"ms_per_iter\": ((t1 - t0) * 1000) / it,\n",
        "}\n",
        "\n",
        "print(f\"CPU method completed in {it} iterations.\")\n",
        "print(f\"Total time: {cpu_results['total_ms']:.3f} ms\")\n",
        "\n",
        "# Save for final analysis\n",
        "%env CPU_RESULTS={json.dumps(cpu_results)}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save CSR to flat binaries for CUDA C++ (float64 weights)\n",
        "import numpy as np, json, os\n",
        "\n",
        "npz = np.load(\"/content/drive/MyDrive/cuda-pagerank/prep/in_csr_webGoogle.npz\", allow_pickle=False)\n",
        "N = int(npz[\"N\"]); M = int(npz[\"M\"])\n",
        "row_ptr = npz[\"row_ptr\"].astype(np.int64, copy=False)\n",
        "col_idx = npz[\"col_idx\"].astype(np.int64, copy=False)\n",
        "val = npz[\"val\"].astype(np.float64, copy=False)\n",
        "outdeg = npz[\"outdeg\"].astype(np.int64, copy=False)\n",
        "\n",
        "outdir = \"/content/drive/MyDrive/cuda-pagerank/bin\"\n",
        "os.makedirs(outdir, exist_ok=True)\n",
        "\n",
        "row_ptr.tofile(f\"{outdir}/row_ptr.bin\")\n",
        "col_idx.tofile(f\"{outdir}/col_idx.bin\")\n",
        "val.tofile(f\"{outdir}/val.bin\")\n",
        "outdeg.tofile(f\"{outdir}/outdeg.bin\")\n",
        "\n",
        "with open(f\"{outdir}/meta.json\", \"w\") as f:\n",
        "    json.dump({\"N\": N, \"M\": M, \"alpha\": 0.85, \"tol\": 1e-6, \"max_iter\": 200}, f)\n",
        "\n",
        "print(\"Saved:\", outdir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kv97El_p3-r-",
        "outputId": "ba4ee6e1-00a9-4bbc-f3e2-2a5cfc1202f3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved: /content/drive/MyDrive/cuda-pagerank/bin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/pagerank_pull.cu\n",
        "#include <cstdio>\n",
        "#include <cstdlib>\n",
        "#include <cstdint>\n",
        "#include <vector>\n",
        "#include <fstream>\n",
        "#include <string>\n",
        "#include <iterator>\n",
        "#include <thrust/device_vector.h>\n",
        "#include <thrust/reduce.h>\n",
        "#include <thrust/transform_reduce.h>\n",
        "#include <thrust/transform.h>\n",
        "#include <thrust/functional.h>\n",
        "#include <thrust/iterator/zip_iterator.h>\n",
        "#include <thrust/tuple.h>\n",
        "#include <cmath>\n",
        "#include <cuda_runtime.h>\n",
        "\n",
        "// A warp-level reduction helper function.\n",
        "__device__ inline double warpReduceSum(double val) {\n",
        "  for (int offset = 16; offset > 0; offset /= 2) {\n",
        "    val += __shfl_down_sync(0xFFFFFFFF, val, offset);\n",
        "  }\n",
        "  return val;\n",
        "}\n",
        "\n",
        "// Method 1: Scalar Kernel. Each thread processes one full row.\n",
        "__global__ void spmv_scalar_kernel(const int64_t* __restrict__ row_ptr,\n",
        "                                   const int64_t* __restrict__ col_idx,\n",
        "                                   const double* __restrict__ val,\n",
        "                                   const double* __restrict__ r,\n",
        "                                   double* __restrict__ tmp,\n",
        "                                   int64_t N) {\n",
        "  int64_t row = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  if (row >= N) return;\n",
        "\n",
        "  double sum = 0.0;\n",
        "  int64_t start = row_ptr[row];\n",
        "  int64_t end = row_ptr[row + 1];\n",
        "\n",
        "  for (int64_t p = start; p < end; ++p) {\n",
        "    sum += val[p] * r[col_idx[p]];\n",
        "  }\n",
        "  tmp[row] = sum;\n",
        "}\n",
        "\n",
        "// Method 2: Vector Kernel. Each warp (32 threads) processes one row.\n",
        "__global__ void spmv_vector_kernel(const int64_t* __restrict__ row_ptr,\n",
        "                                   const int64_t* __restrict__ col_idx,\n",
        "                                   const double* __restrict__ val,\n",
        "                                   const double* __restrict__ r,\n",
        "                                   double* __restrict__ tmp,\n",
        "                                   int64_t N) {\n",
        "  int64_t row = blockIdx.x * blockDim.x + (threadIdx.x / 32);\n",
        "  if (row >= N) return;\n",
        "\n",
        "  int lane_id = threadIdx.x % 32;\n",
        "  double sum = 0.0;\n",
        "  int64_t start = row_ptr[row];\n",
        "  int64_t end = row_ptr[row + 1];\n",
        "\n",
        "  for (int64_t p = start + lane_id; p < end; p += 32) {\n",
        "    sum += val[p] * r[col_idx[p]];\n",
        "  }\n",
        "\n",
        "  sum = warpReduceSum(sum);\n",
        "\n",
        "  if (lane_id == 0) {\n",
        "    tmp[row] = sum;\n",
        "  }\n",
        "}\n",
        "\n",
        "// Helper structs for Thrust reductions\n",
        "struct AbsDiff {\n",
        "  __host__ __device__ double operator()(const thrust::tuple<double,double>& a) const {\n",
        "    return fabs(thrust::get<0>(a) - thrust::get<1>(a));\n",
        "  }\n",
        "};\n",
        "struct MaskPick {\n",
        "  const int64_t* outdeg;\n",
        "  __host__ __device__ double operator()(const thrust::tuple<double,int64_t>& t) const {\n",
        "    return (thrust::get<1>(t)==0) ? thrust::get<0>(t) : 0.0;\n",
        "  }\n",
        "};\n",
        "\n",
        "// PageRank finalization kernel\n",
        "__global__ void finalize_kernel(double* __restrict__ r_new,\n",
        "                                const double* __restrict__ tmp,\n",
        "                                double alpha, double d_ave, double base,\n",
        "                                int64_t N) {\n",
        "  int64_t i = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "  if (i >= N) return;\n",
        "  r_new[i] = alpha * (tmp[i] + d_ave) + base;\n",
        "}\n",
        "\n",
        "// Binary file reader for graph data\n",
        "static void read_bin(const char* path, void* buf, size_t bytes) {\n",
        "  std::ifstream f(path, std::ios::binary);\n",
        "  if(!f) { fprintf(stderr, \"Cannot open %s\\n\", path); std::exit(1); }\n",
        "  f.read(reinterpret_cast<char*>(buf), bytes);\n",
        "  if(!f) { fprintf(stderr, \"Short read on %s\\n\", path); std::exit(1); }\n",
        "}\n",
        "\n",
        "// Generic function to run and benchmark one PageRank method\n",
        "void run_pagerank(const char* method_name,\n",
        "                  void (*spmv_kernel)(const int64_t*, const int64_t*, const double*, const double*, double*, int64_t),\n",
        "                  int64_t N, int max_iter, double tol, double alpha,\n",
        "                  const thrust::device_vector<int64_t>& d_row_ptr,\n",
        "                  const thrust::device_vector<int64_t>& d_col_idx,\n",
        "                  const thrust::device_vector<double>& d_val,\n",
        "                  const thrust::device_vector<int64_t>& d_outdeg,\n",
        "                  dim3 blocks, dim3 threads,\n",
        "                  double& out_ms_total, int& out_iterations, double& out_gbs)\n",
        "{\n",
        "    thrust::device_vector<double> r(N, 1.0 / (double)N);\n",
        "    thrust::device_vector<double> r_new(N, 0.0);\n",
        "    thrust::device_vector<double> tmp(N, 0.0);\n",
        "\n",
        "    const double base = (1.0 - alpha) / (double)N;\n",
        "    double residual = 0.0;\n",
        "    out_ms_total = 0.0;\n",
        "\n",
        "    printf(\"--- Testing %s Method ---\\n\", method_name);\n",
        "    printf(\"Iter | Residual (L1) | Time (ms) | Status\\n\");\n",
        "    printf(\"-----|---------------|-----------|--------\\n\");\n",
        "\n",
        "    cudaEvent_t e0, e1;\n",
        "    cudaEventCreate(&e0); cudaEventCreate(&e1);\n",
        "\n",
        "    for (out_iterations = 1; out_iterations <= max_iter; ++out_iterations) {\n",
        "        cudaEventRecord(e0);\n",
        "\n",
        "        auto zip_dang = thrust::make_zip_iterator(thrust::make_tuple(r.begin(), d_outdeg.begin()));\n",
        "        auto zip_rpair = thrust::make_zip_iterator(thrust::make_tuple(r.begin(), r_new.begin()));\n",
        "\n",
        "        spmv_kernel<<<blocks, threads>>>(\n",
        "            thrust::raw_pointer_cast(d_row_ptr.data()),\n",
        "            thrust::raw_pointer_cast(d_col_idx.data()),\n",
        "            thrust::raw_pointer_cast(d_val.data()),\n",
        "            thrust::raw_pointer_cast(r.data()),\n",
        "            thrust::raw_pointer_cast(tmp.data()),\n",
        "            N);\n",
        "\n",
        "        MaskPick pick{thrust::raw_pointer_cast(d_outdeg.data())};\n",
        "        double d_mass = thrust::transform_reduce(zip_dang, zip_dang + N, pick, 0.0, thrust::plus<double>());\n",
        "        double d_ave = d_mass / (double)N;\n",
        "\n",
        "        finalize_kernel<<< (unsigned)(N + 255)/256, 256 >>>(\n",
        "            thrust::raw_pointer_cast(r_new.data()),\n",
        "            thrust::raw_pointer_cast(tmp.data()),\n",
        "            alpha, d_ave, base, N);\n",
        "\n",
        "        residual = thrust::transform_reduce(zip_rpair, zip_rpair + N, AbsDiff(), 0.0, thrust::plus<double>());\n",
        "        r.swap(r_new);\n",
        "\n",
        "        cudaEventRecord(e1);\n",
        "        cudaEventSynchronize(e1);\n",
        "        float ms = 0.0f;\n",
        "        cudaEventElapsedTime(&ms, e0, e1);\n",
        "        out_ms_total += ms;\n",
        "\n",
        "        if (out_iterations == 1 || out_iterations % 10 == 0 || residual < tol) {\n",
        "            printf(\"%4d | %12.6e | %8.3f | %s\\n\",\n",
        "                   out_iterations, residual, ms,\n",
        "                   (residual < tol) ? \"CONVERGED\" : \"Running\");\n",
        "        }\n",
        "\n",
        "        if (residual < tol) break;\n",
        "    }\n",
        "\n",
        "    // Bandwidth calculation\n",
        "    int64_t M = d_col_idx.size();\n",
        "    const double bytes_per_iter = (double)(M * (sizeof(double) + sizeof(int64_t)) + N * (3 * sizeof(double)));\n",
        "    double ms_per_iter = out_ms_total / out_iterations;\n",
        "    out_gbs = (bytes_per_iter / (ms_per_iter / 1000.0)) / 1e9;\n",
        "\n",
        "    cudaEventDestroy(e0); cudaEventDestroy(e1);\n",
        "    printf(\"Convergence: SUCCESS after %d iterations.\\n\", out_iterations);\n",
        "    printf(\"JSON_RESULT: {\\\"method\\\":\\\"GPU %s\\\",\\\"iterations\\\":%d,\\\"total_ms\\\":%.3f,\\\"ms_per_iter\\\":%.3f,\\\"gbs\\\":%.2f}\\n\\n\",\n",
        "            method_name, out_iterations, out_ms_total, ms_per_iter, out_gbs);\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    printf(\"=== CUDA PageRank: Scalar vs Vector Kernel Comparison ===\\n\\n\");\n",
        "\n",
        "    // Config parsing\n",
        "    std::ifstream mf(\"/content/drive/MyDrive/cuda-pagerank/bin/meta.json\");\n",
        "    std::string s((std::istreambuf_iterator<char>(mf)), std::istreambuf_iterator<char>());\n",
        "    auto getnum = [&](const char* key) -> double {\n",
        "        auto k = s.find(key); if(k == std::string::npos) return 0; k = s.find(':', k); auto e = s.find_first_of(\",}\\n\", k + 1);\n",
        "        return atof(s.substr(k + 1, e - k - 1).c_str());\n",
        "    };\n",
        "    int64_t N = (int64_t)getnum(\"\\\"N\\\"\");\n",
        "    int64_t M = (int64_t)getnum(\"\\\"M\\\"\");\n",
        "    double alpha = getnum(\"\\\"alpha\\\"\");\n",
        "    double tol = getnum(\"\\\"tol\\\"\");\n",
        "    int max_iter = (int)getnum(\"\\\"max_iter\\\"\");\n",
        "\n",
        "    printf(\"Problem Size:\\n\");\n",
        "    printf(\"  Nodes (web pages): %lld\\n\", (long long)N);\n",
        "    printf(\"  Edges (links):     %lld\\n\", (long long)M);\n",
        "    printf(\"  Avg links/page:    %.2f\\n\\n\", (double)M / (double)N);\n",
        "\n",
        "    // Load data from disk\n",
        "    std::vector<int64_t> h_row_ptr(N + 1), h_col_idx(M), h_outdeg(N);\n",
        "    std::vector<double> h_val(M);\n",
        "    read_bin(\"/content/drive/MyDrive/cuda-pagerank/bin/row_ptr.bin\", h_row_ptr.data(), (N + 1) * sizeof(int64_t));\n",
        "    read_bin(\"/content/drive/MyDrive/cuda-pagerank/bin/col_idx.bin\", h_col_idx.data(), M * sizeof(int64_t));\n",
        "    read_bin(\"/content/drive/MyDrive/cuda-pagerank/bin/val.bin\", h_val.data(), M * sizeof(double));\n",
        "    read_bin(\"/content/drive/MyDrive/cuda-pagerank/bin/outdeg.bin\", h_outdeg.data(), N * sizeof(int64_t));\n",
        "\n",
        "    // Transfer data to GPU\n",
        "    thrust::device_vector<int64_t> d_row_ptr = h_row_ptr;\n",
        "    thrust::device_vector<int64_t> d_col_idx = h_col_idx;\n",
        "    thrust::device_vector<int64_t> d_outdeg = h_outdeg;\n",
        "    thrust::device_vector<double> d_val = h_val;\n",
        "\n",
        "    double ms_scalar = 0, ms_vector = 0, gbs_scalar = 0, gbs_vector = 0;\n",
        "    int iter_scalar = 0, iter_vector = 0;\n",
        "\n",
        "    // --- Run Scalar Method ---\n",
        "    int threads_scalar = 256;\n",
        "    int blocks_scalar = (N + threads_scalar - 1) / threads_scalar;\n",
        "    run_pagerank(\"Scalar\", spmv_scalar_kernel, N, max_iter, tol, alpha,\n",
        "                 d_row_ptr, d_col_idx, d_val, d_outdeg,\n",
        "                 dim3(blocks_scalar), dim3(threads_scalar),\n",
        "                 ms_scalar, iter_scalar, gbs_scalar);\n",
        "\n",
        "    // --- Run Vector Method ---\n",
        "    int threads_vector = 256;\n",
        "    int warps_per_block = threads_vector / 32;\n",
        "    int blocks_vector = (N + warps_per_block - 1) / warps_per_block;\n",
        "    run_pagerank(\"Vector\", spmv_vector_kernel, N, max_iter, tol, alpha,\n",
        "                 d_row_ptr, d_col_idx, d_val, d_outdeg,\n",
        "                 dim3(blocks_vector), dim3(threads_vector),\n",
        "                 ms_vector, iter_vector, gbs_vector);\n",
        "\n",
        "    return 0;\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NSjWzN94CGE",
        "outputId": "1508da7d-08c8-4b7f-f22e-6b6cd1ccd7c8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing /content/pagerank_pull.cu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile and run CUDA code\n",
        "!nvcc -O3 -arch=sm_75 /content/pagerank_pull.cu -o /content/pagerank_pull\n",
        "# The 'tee' command saves the output to a file and also prints it\n",
        "!/content/pagerank_pull | tee /content/gpu_results.log"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rayeLaWze19B",
        "outputId": "5c60c95f-71b1-4c3d-aaa3-f019c4d0ef6c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== CUDA PageRank: Scalar vs Vector Kernel Comparison ===\n",
            "\n",
            "Problem Size:\n",
            "  Nodes (web pages): 916428\n",
            "  Edges (links):     5105039\n",
            "  Avg links/page:    5.57\n",
            "\n",
            "--- Testing Scalar Method ---\n",
            "Iter | Residual (L1) | Time (ms) | Status\n",
            "-----|---------------|-----------|--------\n",
            "   1 | 8.508467e-01 |    3.446 | Running\n",
            "  10 | 1.090636e-02 |    3.298 | Running\n",
            "  20 | 1.298404e-03 |    3.322 | Running\n",
            "  30 | 2.066399e-04 |    3.264 | Running\n",
            "  40 | 3.607447e-05 |    3.207 | Running\n",
            "  50 | 6.557848e-06 |    2.947 | Running\n",
            "  60 | 1.219864e-06 |    2.993 | Running\n",
            "  62 | 8.729468e-07 |    2.972 | CONVERGED\n",
            "Convergence: SUCCESS after 62 iterations.\n",
            "JSON_RESULT: {\"method\":\"GPU Scalar\",\"iterations\":62,\"total_ms\":197.060,\"ms_per_iter\":3.178,\"gbs\":32.62}\n",
            "\n",
            "--- Testing Vector Method ---\n",
            "Iter | Residual (L1) | Time (ms) | Status\n",
            "-----|---------------|-----------|--------\n",
            "   1 | 6.913818e-01 |    0.737 | Running\n",
            "  10 | 4.030835e-07 |    0.682 | CONVERGED\n",
            "Convergence: SUCCESS after 10 iterations.\n",
            "JSON_RESULT: {\"method\":\"GPU Vector\",\"iterations\":10,\"total_ms\":6.962,\"ms_per_iter\":0.696,\"gbs\":148.92}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Analysis\n",
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# List to hold all results\n",
        "results = []\n",
        "\n",
        "# Get CPU results from environment variable\n",
        "cpu_results_str = os.environ.get('CPU_RESULTS')\n",
        "if cpu_results_str:\n",
        "    cpu_data = json.loads(cpu_results_str)\n",
        "    cpu_data['gbs'] = 'N/A'  # Bandwidth not measured for CPU\n",
        "    results.append(cpu_data)\n",
        "\n",
        "# Parse GPU results from the log file\n",
        "with open('/content/gpu_results.log', 'r') as f:\n",
        "    for line in f:\n",
        "        if line.startswith('JSON_RESULT:'):\n",
        "            # Extract the JSON part of the string\n",
        "            json_str = line.replace('JSON_RESULT: ', '').strip()\n",
        "            results.append(json.loads(json_str))\n",
        "\n",
        "# Create a pandas DataFrame for nice formatting\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "# Calculate speedup relative to the CPU\n",
        "if not df[df['method'] == 'CPU (NumPy)'].empty:\n",
        "    cpu_time_per_iter = df[df['method'] == 'CPU (NumPy)']['ms_per_iter'].iloc[0]\n",
        "    df['speedup'] = (cpu_time_per_iter / df['ms_per_iter']).round(2)\n",
        "else:\n",
        "    df['speedup'] = 'N/A'\n",
        "\n",
        "# Format and display the final table\n",
        "df_display = df[['method', 'iterations', 'total_ms', 'ms_per_iter', 'gbs', 'speedup']]\n",
        "df_display = df_display.rename(columns={\n",
        "    'method': 'Implementation',\n",
        "    'iterations': 'Iterations',\n",
        "    'total_ms': 'Total Time (ms)',\n",
        "    'ms_per_iter': 'Time / Iter (ms)',\n",
        "    'gbs': 'Bandwidth (GB/s)',\n",
        "    'speedup': 'Speedup vs CPU'\n",
        "})\n",
        "\n",
        "# Display the results\n",
        "print(\"=== Overall Performance Analysis ===\")\n",
        "display(df_display.style.format({\n",
        "    'Total Time (ms)': '{:.2f}',\n",
        "    'Time / Iter (ms)': '{:.3f}',\n",
        "}).hide(axis=\"index\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 161
        },
        "id": "mNgrBPc3e7Mj",
        "outputId": "679b3e69-46ba-498e-e034-ac2f445c34b6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Overall Performance Analysis ===\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x7bf511f65190>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_09d70\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th id=\"T_09d70_level0_col0\" class=\"col_heading level0 col0\" >Implementation</th>\n",
              "      <th id=\"T_09d70_level0_col1\" class=\"col_heading level0 col1\" >Iterations</th>\n",
              "      <th id=\"T_09d70_level0_col2\" class=\"col_heading level0 col2\" >Total Time (ms)</th>\n",
              "      <th id=\"T_09d70_level0_col3\" class=\"col_heading level0 col3\" >Time / Iter (ms)</th>\n",
              "      <th id=\"T_09d70_level0_col4\" class=\"col_heading level0 col4\" >Bandwidth (GB/s)</th>\n",
              "      <th id=\"T_09d70_level0_col5\" class=\"col_heading level0 col5\" >Speedup vs CPU</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td id=\"T_09d70_row0_col0\" class=\"data row0 col0\" >CPU (NumPy)</td>\n",
              "      <td id=\"T_09d70_row0_col1\" class=\"data row0 col1\" >62</td>\n",
              "      <td id=\"T_09d70_row0_col2\" class=\"data row0 col2\" >210301.53</td>\n",
              "      <td id=\"T_09d70_row0_col3\" class=\"data row0 col3\" >3391.960</td>\n",
              "      <td id=\"T_09d70_row0_col4\" class=\"data row0 col4\" >N/A</td>\n",
              "      <td id=\"T_09d70_row0_col5\" class=\"data row0 col5\" >1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_09d70_row1_col0\" class=\"data row1 col0\" >GPU Scalar</td>\n",
              "      <td id=\"T_09d70_row1_col1\" class=\"data row1 col1\" >62</td>\n",
              "      <td id=\"T_09d70_row1_col2\" class=\"data row1 col2\" >197.06</td>\n",
              "      <td id=\"T_09d70_row1_col3\" class=\"data row1 col3\" >3.178</td>\n",
              "      <td id=\"T_09d70_row1_col4\" class=\"data row1 col4\" >32.620000</td>\n",
              "      <td id=\"T_09d70_row1_col5\" class=\"data row1 col5\" >1067.330000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_09d70_row2_col0\" class=\"data row2 col0\" >GPU Vector</td>\n",
              "      <td id=\"T_09d70_row2_col1\" class=\"data row2 col1\" >10</td>\n",
              "      <td id=\"T_09d70_row2_col2\" class=\"data row2 col2\" >6.96</td>\n",
              "      <td id=\"T_09d70_row2_col3\" class=\"data row2 col3\" >0.696</td>\n",
              "      <td id=\"T_09d70_row2_col4\" class=\"data row2 col4\" >148.920000</td>\n",
              "      <td id=\"T_09d70_row2_col5\" class=\"data row2 col5\" >4873.510000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNth2tSWiCFc+kSeq3MBddv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}